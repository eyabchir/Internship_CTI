{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c67ca55-b05a-42f2-9a8b-39989249ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        key, value = line.split('=', 1)\n",
    "        return value\n",
    "\n",
    "# Use the API key\n",
    "GROQ_API_KEY = get_api_key('api_key_llama2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039e892-e3bc-4a65-b74e-a845f40e8f20",
   "metadata": {},
   "source": [
    "#### Generation des réponses de Llama3 sur dataset RCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32fa6b3e-3fc3-4904-abdb-822df8e07ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Prompts: 100%|████████████████████████████████████████████████████████████| 400/400 [18:58<00:00,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved successfully in: llama3_cti_rcm_R.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to read the API key\n",
    "def get_api_key(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        key, value = line.split('=', 1)\n",
    "        return value\n",
    "\n",
    "# Use the API key\n",
    "GROQ_API_KEY = get_api_key('api_key_llama2.txt')\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Load the cti-rcm dataset\n",
    "df = pd.read_csv('cti-rcm.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "# Select only the 'Prompt' column\n",
    "df_prompts = df[['Prompt']]  # Ensure the column exists in your dataset\n",
    "\n",
    "# Define the range of lines to process\n",
    "start_line = 0  # Adjust start line\n",
    "end_line = 400\n",
    "df_limited = df_prompts.iloc[start_line:end_line]\n",
    "\n",
    "# Path to the output TSV file\n",
    "output_file = 'llama3_cti_rcm_R.tsv'\n",
    "\n",
    "# Write responses to the TSV file\n",
    "with open(output_file, 'a') as file:\n",
    "    \n",
    "    # Iterate through each row in the dataset\n",
    "    for index, row in tqdm(df_limited.iterrows(), total=df_limited.shape[0], desc=\"Processing Prompts\"):\n",
    "        # Extract the prompt\n",
    "        prompt = row['Prompt']\n",
    "\n",
    "        # Get the response from the llama3 model using Groq\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=1,\n",
    "                max_tokens=50,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            # Extract and clean the response\n",
    "            response_text = completion.choices[0].message.content.strip()\n",
    "            cleaned_response = response_text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "            # Write the cleaned response to the output TSV file\n",
    "            file.write(f'{cleaned_response}\\n')\n",
    "\n",
    "            # Optional: Sleep to avoid rate-limiting\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index + 1}: {e}\")\n",
    "            file.write('ERROR\\n')  # Log errors as 'ERROR' in the output file\n",
    "\n",
    "print(\"Responses saved successfully in:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2868de-27c9-4530-9687-3f0956a9d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Prompts:  80%|████████████████████████████████████████████████▎           | 322/400 [14:07<03:41,  2.84s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to read the API key\n",
    "def get_api_key(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        line = file.readline().strip()\n",
    "        key, value = line.split('=', 1)\n",
    "        return value\n",
    "\n",
    "# Use the API key\n",
    "GROQ_API_KEY = get_api_key('api_key_llama2.txt')\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Load the cti-rcm dataset\n",
    "df = pd.read_csv('cti-rcm.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "# Select only the 'Prompt' column\n",
    "df_prompts = df[['Prompt']]  # Ensure the column exists in your dataset\n",
    "\n",
    "# Define the range of lines to process\n",
    "start_line = 400  # Adjust start line\n",
    "end_line = 800\n",
    "df_limited = df_prompts.iloc[start_line:end_line]\n",
    "\n",
    "# Path to the output TSV file\n",
    "output_file = 'llama3_cti_rcm_R2.tsv'\n",
    "\n",
    "# Write responses to the TSV file\n",
    "with open(output_file, 'a') as file:\n",
    "    \n",
    "    # Iterate through each row in the dataset\n",
    "    for index, row in tqdm(df_limited.iterrows(), total=df_limited.shape[0], desc=\"Processing Prompts\"):\n",
    "        # Extract the prompt\n",
    "        prompt = row['Prompt']\n",
    "\n",
    "        # Get the response from the llama3 model using Groq\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=1,\n",
    "                max_tokens=50,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "            # Extract and clean the response\n",
    "            response_text = completion.choices[0].message.content.strip()\n",
    "            cleaned_response = response_text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "            # Write the cleaned response to the output TSV file\n",
    "            file.write(f'{cleaned_response}\\n')\n",
    "\n",
    "            # Optional: Sleep to avoid rate-limiting\n",
    "            time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index + 1}: {e}\")\n",
    "            file.write('ERROR\\n')  # Log errors as 'ERROR' in the output file\n",
    "\n",
    "print(\"Responses saved successfully in:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae8878-8157-4973-a3b5-3c5a92b9b7ce",
   "metadata": {},
   "source": [
    "#### Extraction de la réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d31752-491d-44e9-82d5-639cf74dbe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction terminée. Les résultats sont sauvegardés dans Llamma3_reponses_rcm.tsv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Charger la dataset\n",
    "input_file = 'llama3_cti_rcm_R.tsv'\n",
    "df = pd.read_csv(input_file, sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "# Fonction pour extraire les expressions commençant par \"CWE-\"\n",
    "def extract_cwe(text):\n",
    "    # Utiliser une expression régulière pour trouver les mentions de CWE-\n",
    "    matches = re.findall(r'\\bCWE-\\d+', str(text))\n",
    "    # Retourner la première correspondance ou \"N/A\" si aucune trouvée\n",
    "    return matches[0] if matches else \"N/A\"\n",
    "\n",
    "# Appliquer la fonction à chaque ligne de la colonne contenant les réponses\n",
    "df['Extracted_CWE'] = df['Responses'].apply(extract_cwe)  \n",
    "\n",
    "# Créer une nouvelle dataset avec la colonne CWE extraite\n",
    "Llamma3_reponses_rcm = df[['Responses', 'Extracted_CWE']]\n",
    "\n",
    "# Sauvegarder la nouvelle dataset dans un fichier TSV\n",
    "output_file = 'Llamma3_reponses_rcm.tsv'\n",
    "Llamma3_reponses_rcm.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Extraction terminée. Les résultats sont sauvegardés dans {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2cdee-550c-4585-a190-3252b7fcc9e2",
   "metadata": {},
   "source": [
    "#### Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2afc906b-3b4f-434a-8101-e8ec85df84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de réponses correctes : 38.75%\n",
      "Les résultats de la comparaison ont été sauvegardés dans comparison_results.tsv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "responses_file = 'Llamma3_reponses_rcm.tsv'\n",
    "cti_file = 'cti-rcm-responses.tsv' #Llama3 responses , dataset of the benchmark\n",
    "\n",
    "df_responses = pd.read_csv(responses_file, sep='\\t', encoding='ISO-8859-1')\n",
    "df_cti = pd.read_csv(cti_file, sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "df_responses_limited = df_responses.head(400)\n",
    "df_cti_limited = df_cti.head(400)\n",
    "\n",
    "# Comparer les deux colonnes\n",
    "comparison = df_responses_limited['Extracted_CWE'] == df_cti_limited['LLAMA3-8B']\n",
    "\n",
    "# Calculer le pourcentage de correspondance\n",
    "percentage_correct = (comparison.sum() / len(comparison)) * 100\n",
    "\n",
    "print(f\"Pourcentage de réponses correctes : {percentage_correct:.2f}%\")\n",
    "\n",
    "# Créer un nouveau DataFrame pour afficher les comparaisons\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Llamma3_Responses': df_responses_limited['Extracted_CWE'],\n",
    "    'CTI_Llamma3': df_cti_limited['LLAMA3-8B'],\n",
    "    'Is_Correct': comparison\n",
    "})\n",
    "\n",
    "# Sauvegarder les résultats de la comparaison dans un fichier\n",
    "output_file = 'comparison_results.tsv'\n",
    "comparison_results.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Les résultats de la comparaison ont été sauvegardés dans {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
